{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 : https://github.com/qbxlvnf11/machine-learning-basic/blob/master/Pytorch_Backpropagation.ipynb\n",
    "\n",
    "## pytorch function module\n",
    " \n",
    "- torch.nn : 파이토치, 인공 신경망 모델을 가짐\n",
    "\n",
    "- torch.optim : 모델 최적화\n",
    "\n",
    "- torch.nn.functional : nn 모듈의 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input & Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input\n",
    "input_tensor = torch.tensor([0.2, 0.1], dtype=torch.float64)\n",
    "\n",
    "# Define weights : w1, w2, b1, b2\n",
    "w1 = nn.Embedding(2, 2, dtype=torch.float64)\n",
    "w2 = nn.Embedding(2, 2, dtype=torch.float64)\n",
    "b1 = nn.Embedding(1, 2, dtype=torch.float64)\n",
    "b2 = nn.Embedding(1, 2, dtype=torch.float64)\n",
    "\n",
    "# init weights : w1, w2, b1, b2\n",
    "w1.weight.data = torch.tensor([[0.2, 0.1], [0.4, 0.15]], dtype=torch.float64, requires_grad=True).t()\n",
    "w2.weight.data = torch.tensor([[0.65, 0.7], [0.45, 0.3]], dtype=torch.float64, requires_grad=True).t()\n",
    "b1.weight.data = torch.tensor([[0.3]], dtype=torch.float64, requires_grad=True).t()\n",
    "b2.weight.data = torch.tensor([[0.5]], dtype=torch.float64, requires_grad=True).t()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "input_tensor: tensor([0.2000, 0.1000], dtype=torch.float64)\n",
      "******************************\n",
      "w1.weight: Parameter containing:\n",
      "tensor([[0.2000, 0.4000],\n",
      "        [0.1000, 0.1500]], dtype=torch.float64, requires_grad=True)\n",
      "w1.weight.grad: None\n",
      "b1.weight: Parameter containing:\n",
      "tensor([[0.3000]], dtype=torch.float64, requires_grad=True)\n",
      "b1.weight.grad: None\n",
      "******************************\n",
      "w2.weight: Parameter containing:\n",
      "tensor([[0.6500, 0.4500],\n",
      "        [0.7000, 0.3000]], dtype=torch.float64, requires_grad=True)\n",
      "w2.weight.grad: None\n",
      "b2.weight: Parameter containing:\n",
      "tensor([[0.6500, 0.4500],\n",
      "        [0.7000, 0.3000]], dtype=torch.float64, requires_grad=True)\n",
      "b2.weight.grad: None\n",
      "b2.weight: Parameter containing:\n",
      "tensor([[0.5000]], dtype=torch.float64, requires_grad=True)\n",
      "b2.weight.grad: None\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# print weights\n",
    "print('*'*30)\n",
    "print('input_tensor:', input_tensor)\n",
    "print('*'*30)\n",
    "print('w1.weight:', w1.weight)\n",
    "\n",
    "print('w1.weight.grad:', w1.weight.grad)\n",
    "print('b1.weight:', b1.weight)\n",
    "print('b1.weight.grad:', b1.weight.grad)\n",
    "print('*'*30)\n",
    "print('w2.weight:', w2.weight)\n",
    "print('w2.weight.grad:', w2.weight.grad)\n",
    "print('b2.weight:', w2.weight)\n",
    "print('b2.weight.grad:', w2.weight.grad)\n",
    "print('b2.weight:', b2.weight)\n",
    "print('b2.weight.grad:', b2.weight.grad)\n",
    "print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer(MLP)\n",
    "net_h1_h2 = torch.matmul(input_tensor, w1.weight) + b1.weight\n",
    "out_h1_h2 = F.sigmoid(net_h1_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_h1_h2: tensor([[0.3500, 0.3950]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "out_h1_h2: tensor([[0.5866, 0.5975]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "out_h1_h2.grad: None\n"
     ]
    }
   ],
   "source": [
    "# [[net_h1, net_h2]]\n",
    "print('net_h1_h2:', net_h1_h2)\n",
    "\n",
    "# [[out_h1, out_h2]]\n",
    "print('out_h1_h2:', out_h1_h2)\n",
    "print('out_h1_h2.grad:', out_h1_h2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer(MLP)\n",
    "net_o1_o2 = torch.matmul(out_h1_h2, w2.weight) + b2.weight\n",
    "out_o1_o2 = F.sigmoid(net_o1_o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_o1_o2: tensor([[1.2995, 0.9432]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "out_o1_o2: tensor([[0.7858, 0.7198]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "out_o1_o2.grad: None\n"
     ]
    }
   ],
   "source": [
    "# [[net_h1, net_h2]]\n",
    "print('net_o1_o2:', net_o1_o2)\n",
    "# [[out_h1, out_h2]]\n",
    "print('out_o1_o2:', out_o1_o2)\n",
    "print('out_o1_o2.grad:', out_o1_o2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor([0.99, 0.01], dtype=torch.float64, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss = torch.sum(0.5*torch.square(label - out_o1_o2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.2727, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gradient of each weight & bias\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.weight.grad: tensor([[0.0020, 0.0009],\n",
      "        [0.0010, 0.0005]], dtype=torch.float64)\n",
      "b1.weight.grad: tensor([[0.0147]], dtype=torch.float64)\n",
      "w2.weight.grad: tensor([[-0.0202,  0.0840],\n",
      "        [-0.0205,  0.0855]], dtype=torch.float64)\n",
      "b2.weight.grad: tensor([[0.1088]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Gradients\n",
    "# Save gradients in weight.grad attribute\n",
    "print('w1.weight.grad:', w1.weight.grad)\n",
    "print('b1.weight.grad:', b1.weight.grad)\n",
    "print('w2.weight.grad:', w2.weight.grad)\n",
    "print('b2.weight.grad:', b2.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- Optimization (1 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.weight: Parameter containing:\n",
      "tensor([[0.2000, 0.4000],\n",
      "        [0.1000, 0.1500]], dtype=torch.float64, requires_grad=True)\n",
      "b1.weight Parameter containing:\n",
      "tensor([[0.3000]], dtype=torch.float64, requires_grad=True)\n",
      "w2.weight Parameter containing:\n",
      "tensor([[0.6500, 0.4500],\n",
      "        [0.7000, 0.3000]], dtype=torch.float64, requires_grad=True)\n",
      "b2.weight Parameter containing:\n",
      "tensor([[0.5000]], dtype=torch.float64, requires_grad=True)\n",
      "loss: tensor(0.2727, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "label: tensor([0.9900, 0.0100], dtype=torch.float64, requires_grad=True)\n",
      "output: tensor([[0.7858, 0.7198]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Before optimization\n",
    "# Weights\n",
    "print('w1.weight:', w1.weight)\n",
    "print('b1.weight', b1.weight)\n",
    "print('w2.weight', w2.weight)\n",
    "print('b2.weight', b2.weight)\n",
    "\n",
    "# Loss\n",
    "h1 = F.sigmoid(torch.matmul(input_tensor, w1.weight) + b1.weight)\n",
    "output = F.sigmoid(torch.matmul(h1, w2.weight) + b2.weight)\n",
    "print('loss:', torch.sum(0.5*torch.square(label - output)))\n",
    "\n",
    "# output\n",
    "print('label:', label)\n",
    "print('output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 0.5 \n",
    "# optimizer\n",
    "optimizer = optim.SGD((w1.weight, w2.weight, b1.weight, b2.weight), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.weight: Parameter containing:\n",
      "tensor([[0.1990, 0.3995],\n",
      "        [0.0995, 0.1498]], dtype=torch.float64, requires_grad=True)\n",
      "b1.weight Parameter containing:\n",
      "tensor([[0.2926]], dtype=torch.float64, requires_grad=True)\n",
      "w2.weight Parameter containing:\n",
      "tensor([[0.6601, 0.4080],\n",
      "        [0.7103, 0.2572]], dtype=torch.float64, requires_grad=True)\n",
      "b2.weight: Parameter containing:\n",
      "tensor([[0.4456]], dtype=torch.float64, requires_grad=True)\n",
      "loss: tensor(0.2591, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "label: tensor([0.9900, 0.0100], dtype=torch.float64, requires_grad=True)\n",
      "output: tensor([[0.7781, 0.6979]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Optimization(1 epoch)\n",
    "# Optimizing weights\n",
    "print('w1.weight:', w1.weight)\n",
    "print('b1.weight', b1.weight)\n",
    "print('w2.weight', w2.weight)\n",
    "print('b2.weight:', b2.weight)\n",
    "\n",
    "# Decreasing loss\n",
    "h1 = F.sigmoid(torch.matmul(input_tensor, w1.weight) + b1.weight)\n",
    "output = F.sigmoid(torch.matmul(h1, w2.weight) + b2.weight)\n",
    "print('loss:', torch.sum(0.5*torch.square(label -output)))\n",
    "# More optimizing output\n",
    "print('label:', label)\n",
    "print('output:', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- Optimization(1000 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.2591, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0095, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0034, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0019, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0013, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0009, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0007, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0006, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0003, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1000 epochs\n",
    "for i in range(1, 1001):\n",
    "\n",
    "    # init gradient of optimizer\n",
    "    # If this method not called, gradient is stacjed\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    h1 = F.sigmoid(torch.matmul(input_tensor, w1.weight) + b1.weight)\n",
    "    output = F.sigmoid(torch.matmul(h1, w2.weight) + b2.weight)\n",
    "\n",
    "    # Loss\n",
    "    loss = torch.sum(0.5 *torch.square(label - output))\n",
    "\n",
    "    if i == 1 or i % 100 == 0:\n",
    "        # Decreasing loss\n",
    "        print('loss:', loss)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimization\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: tensor([0.9900, 0.0100], dtype=torch.float64, requires_grad=True)\n",
      "output: tensor([[0.9717, 0.0287]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# validation of output(1000 epochs)\n",
    "\n",
    "h1 = F.sigmoid(torch.matmul(input_tensor, w1.weight) + b1.weight)\n",
    "output = F.sigmoid(torch.matmul(h1, w2.weight) + b2.weight)\n",
    "\n",
    "# Output : close to the label\n",
    "print('label:', label)\n",
    "print('output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
